{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f735074-32e0-42c0-977c-0ca6cb078f84",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DATA PROCESSING\n",
    "(Convert all data in xlfiles/ to db.json, used as the website database)\n",
    "\n",
    "Output from find_aas.py is referred to as \"aa data\" and from franken_pas.m as \"franken data\"\n",
    "Each bullet point corresponds to a single cell, in order\n",
    "- convert .xlsx files in xlfiles/ to .csv files in aas_out/ (find_aas.py output) and franken_out/\n",
    "- convert the aa data csv files to json\n",
    "- convert the franken data csv files to json\n",
    "- create the data dictionary and put json data in\n",
    "- get fold data from SCOP and write the fold for each Viper pdb_id to the data dictionary \n",
    "- write RCSB data from families.xlsx and id_genus.txt to data dictionary\n",
    "- write the gauge points from the point data\n",
    "- rename some ViperDB genomes for simplicity and consistency purposes\n",
    "- write aa data of full capsid to data dictionary (aas.json is made by count_aas.py)\n",
    "- write atom count data (id_atoms.txt is made by analyze_aas.ipynb)\n",
    "- compile data about the closest aa (& other aa within 5 Angstroms) for the closest point for the single top line in franken output, along with other aa data described in the README\n",
    "- separate into discrete and continuous properties, change data types accordingly, and add the lists to dictionary\n",
    "- create filters for sorting discrete variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50356c8d-bacc-4546-a56e-e80586e64005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all xl files to csv\n",
    "import openpyxl\n",
    "import csv\n",
    "import json \n",
    "import sys\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "Path(\"./aas_out/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./franken_out/\").mkdir(parents=True, exist_ok=True)\n",
    "for filename in os.listdir('xlfiles'):\n",
    "    if '.xlsx' not in filename: continue\n",
    "    wb = openpyxl.load_workbook('xlfiles/' + filename) \n",
    "    if 'full' in filename:\n",
    "        for name in wb.sheetnames:\n",
    "            sh = wb[name]\n",
    "            v = filename.split('full_')[-1].split('.')[0]\n",
    "            Path(\"./aas_out/\" + v + '/').mkdir(parents=True, exist_ok=True)\n",
    "            with open('aas_out/' + v + '/' + name + '.csv', 'w', newline=\"\") as file_handle:\n",
    "                csv_writer = csv.writer(file_handle)\n",
    "                for row in sh.iter_rows(): # generator; was sh.rows\n",
    "                    csv_writer.writerow([cell.value for cell in row])\n",
    "    else:\n",
    "        sh = wb.active \n",
    "        with open('franken_out/' + filename.split('.')[0] + '.csv', 'w', newline=\"\") as file_handle:\n",
    "            csv_writer = csv.writer(file_handle)\n",
    "            for row in sh.iter_rows(): # generator; was sh.rows\n",
    "                csv_writer.writerow([cell.value for cell in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4720cfd2-790b-4449-b1ab-f043b53441d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert aa csv files to json\n",
    "def aa_csv_to_json(csvFilePath, jsonFilePath):\n",
    "    chain_dict = {}\n",
    "    with open(csvFilePath, encoding='utf-8') as csvf: \n",
    "            chain_dict = {}\n",
    "            x = [l for l in csvf]\n",
    "            ixs = [m.start() for m in re.finditer('Chain', x[0])]\n",
    "            chain_titles = [x[0][i:i+7] for i in ixs]\n",
    "            for i, c in enumerate(chain_titles):\n",
    "                c_buff = i*4\n",
    "                for j in range(2, len(x)):\n",
    "                    if c not in chain_dict.keys():\n",
    "                        chain_dict[c] = []\n",
    "                    c_dict = {}\n",
    "                    c_dict['Point'] = x[j].split(',')[0]\n",
    "                    c_dict['Distance'] = x[j].split(',')[c_buff + 1]\n",
    "                    c_dict['Atom'] = x[j].split(',')[c_buff + 2]\n",
    "                    c_dict['AARN'] = x[j].split(',')[c_buff + 3]\n",
    "                    c_dict['Other AA'] = x[j].split(',')[c_buff + 4]\n",
    "                    chain_dict[c].append(c_dict)\n",
    "  \n",
    "    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf: \n",
    "        jsonString = json.dumps(chain_dict, indent=4)\n",
    "        jsonf.write(jsonString)\n",
    "\n",
    "\n",
    "for filename in os.listdir('aas_out'):\n",
    "    for csv_file in os.listdir('aas_out/' + filename + '/'):\n",
    "        try:\n",
    "            Path(\"aas_json/\" + filename).mkdir(parents=True, exist_ok=True)\n",
    "            csvFilePath = 'aas_out/' + filename + '/' + csv_file\n",
    "            jsonFilePath = 'aas_json/' + filename + '/' + csv_file.split('.')[0] + '.json'\n",
    "            aa_csv_to_json(csvFilePath, jsonFilePath)\n",
    "        except Exception as e:\n",
    "            print(csv_file)\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b14e6ee-4283-42d2-9038-af0aabe35483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert franken csv files to json\n",
    "def franken_csv_to_json(csvFilePath, jsonFilePath):\n",
    "    jsonArray = []\n",
    "      \n",
    "    #read csv file\n",
    "    with open(csvFilePath, encoding='utf-8') as csvf: \n",
    "        #load csv file data using csv library's dictionary reader\n",
    "        csvReader = csv.DictReader(csvf) \n",
    "\n",
    "        #convert each csv row into python dict\n",
    "        for row in csvReader: \n",
    "            #add this python dict to json array\n",
    "            jsonArray.append(row)\n",
    "  \n",
    "    #convert python jsonArray to JSON String and write to file\n",
    "    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf: \n",
    "        jsonString = json.dumps(jsonArray, indent=4)\n",
    "        jsonf.write(jsonString)\n",
    "        \n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "Path(\"franken_json/\").mkdir(parents=True, exist_ok=True)\n",
    "for csv_file in os.listdir('franken_out'):\n",
    "    try:\n",
    "        filename =csv_file.split('.')[0]\n",
    "        csvFilePath = 'franken_out/' + csv_file\n",
    "        jsonFilePath = 'franken_json/' + filename + '.json'\n",
    "        franken_csv_to_json(csvFilePath, jsonFilePath)\n",
    "    except Exception as e:\n",
    "        print(csv_file)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd2ac7c-e350-4325-a6ea-c7447b42cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put json data into dictionary\n",
    "data = {'data': {}}\n",
    "for v_dir in os.listdir('aas_json'):\n",
    "    pa_dict = {}\n",
    "    for json_file in os.listdir('aas_json/' + v_dir + '/'):\n",
    "        json_path = 'aas_json/' + v_dir + '/' + json_file\n",
    "        with open(json_path, 'r') as f:\n",
    "            pa_data = json.load(f)\n",
    "            pa_dict[json_file.split('_')[-1].split('.')[0]] = pa_data\n",
    "    if v_dir not in data['data'].keys(): data['data'][v_dir] = {}\n",
    "    data['data'][v_dir]['point_arrays'] = pa_dict\n",
    "    \n",
    "for v_dir in os.listdir('franken_json'):\n",
    "    pa_dict = {}\n",
    "    v = v_dir.split('.')[0]\n",
    "    json_path = 'franken_json/' + v_dir\n",
    "    with open(json_path, 'r') as f:\n",
    "        franken_data = json.load(f)\n",
    "    if v not in data['data'].keys(): data['data'][v] = {}\n",
    "    data['data'][v]['points'] = franken_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99db5d7a-51c2-46f7-a07c-9b22f82ba758",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get fold data for each virus from SCOP and write it to json\n",
    "fold_trans.txt contains all SCOP codes which map to each fold as described in Nasir\n",
    "this was done by finding the fold categories corresponding to folds in Nasir as they describe,\n",
    "then getting the SCOP IDS which map to each of those categories\n",
    "all_ids.txt contains each pdb_id on Viper (the code to create it is no longer working bc of API changes)\n",
    "'''\n",
    "# SCOP DOWNLOAD LINKS\n",
    "#http://scop.mrc-lmb.cam.ac.uk/download\n",
    "\n",
    "#SCOP domain definitions and classification.\n",
    "#File: scop-cla-latest.txt\n",
    "scop_url = 'data/scop.txt'\n",
    "\n",
    "#SCOP node descriptions.\n",
    "#File: scop-des-latest.txt\n",
    "des_file = 'data/scop-des.txt'\n",
    "\n",
    "#SCOP represented structures.\n",
    "#File: scop-represented-structures-latest.txt\n",
    "scop_repr = 'data/scop-repr.txt'\n",
    "fold_dict = {}\n",
    "\n",
    "with open('data/fold_trans.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        fid = l.split()[0]\n",
    "        ftrans = l.split(' ',1)[-1].replace('\\n', '')\n",
    "        fold_dict[fid] = ftrans\n",
    "\n",
    "with open('data/all_ids.txt', 'r') as f:\n",
    "    pdb_ids = [l.replace('\\n', '').upper() for l in f]\n",
    "\n",
    "try:\n",
    "    pdb_ids.remove('1E6T')\n",
    "    pdb_ids.append('2BU1')\n",
    "    pdb_ids.remove('3IYU')\n",
    "    pdb_ids.append('4V7Q')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "\n",
    "with open(scop_url, 'r') as f:\n",
    "    id_res = {l.split(' ')[1]:l.split(' ')[-1] for l in f if l[0] != '#'}\n",
    "with open(scop_url, 'r') as f:\n",
    "    uid_res = {l.split(' ')[0]:l.split(' ')[-1] for l in f if l[0] != '#'}\n",
    "with open(des_file, 'r') as f:\n",
    "    des_dict = {l.split(' ')[0]: l.split(' ', 1)[1] for l in f if l[0]!='#'}\n",
    "\n",
    "id_fold = {}\n",
    "for i in pdb_ids:\n",
    "    if i in id_res:\n",
    "        ires = id_res[i]\n",
    "        for sid, trans in fold_dict.items():\n",
    "            if sid in ires:\n",
    "                id_fold[i] = trans\n",
    "\n",
    "with open(scop_repr, 'r') as f:\n",
    "    repr_lst = [l.replace('\\n', '').split(' ') for l in f]\n",
    "\n",
    "id_unident_uid = {}\n",
    "for i in repr_lst:\n",
    "    if i[0] not in id_unident_uid.keys():\n",
    "        id_unident_uid[i[0]] = []\n",
    "    id_unident_uid[i[0]].append(i[2])\n",
    "    \n",
    "for i in pdb_ids:\n",
    "    if i.upper() not in id_fold and i.upper() in id_unident_uid.keys():\n",
    "        for uid in id_unident_uid[i.upper()]:\n",
    "            if uid in uid_res:\n",
    "                res = uid_res[uid]\n",
    "                for fld in fold_dict:\n",
    "                    if fld in res:\n",
    "                        id_fold[i] = fold_dict[fld]\n",
    "                        continue\n",
    "try:\n",
    "    id_fold['1E6T'] = id_fold.pop('2BU1')\n",
    "    id_fold['3IYU'] = id_fold.pop('4V7Q')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "with open('data/id_fold.txt', 'w') as f:\n",
    "    for k, v in id_fold.items():\n",
    "        f.write(k.lower() + ' ' + v + '\\n')\n",
    "\n",
    "with open('id_fold.txt', 'r') as f:\n",
    "    id_fold = {l.split()[0]: l.replace(l.split()[0], '')[1:].replace('\\n','') for l in f}\n",
    "for k,v in id_fold.items():\n",
    "    if k not in data['data'].keys(): data['data'][k] = {}\n",
    "    data['data'][k]['fold'] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd9b6ff7-484b-4e0c-ae0c-704f538803ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write RCSB data from families.xlsx to data dictionary\n",
    "#The code used to make families.xlsx and id_genus.txt is no longer working bc of API changes but is in family_stats.ipynb\n",
    "Path(\"./fams/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./json_fams/\").mkdir(parents=True, exist_ok=True)\n",
    "filename = 'data/families.xlsx'\n",
    "wb = openpyxl.load_workbook(filename) \n",
    "for name in wb.sheetnames:\n",
    "    sh = wb[name]\n",
    "    with open('fams/' + name + '.csv', 'w', newline=\"\") as file_handle:\n",
    "        csv_writer = csv.writer(file_handle)\n",
    "        for row in sh.iter_rows(): # generator; was sh.rows\n",
    "            csv_writer.writerow([cell.value for cell in row])\n",
    "\n",
    "for filename in os.listdir('fams'):\n",
    "    if '.csv' not in filename: continue\n",
    "    fam = filename.split('.')[0]\n",
    "    csvFilePath = 'fams/' + fam + '.csv'\n",
    "    jsonFilePath = 'json_fams/' + fam + '.json'\n",
    "    def csv_to_json(csvFilePath, jsonFilePath):\n",
    "        jsonArray = []\n",
    "\n",
    "        #read csv file\n",
    "        with open(csvFilePath, encoding='utf-8') as csvf: \n",
    "            #load csv file data using csv library's dictionary reader\n",
    "            csvReader = csv.DictReader(csvf) \n",
    "\n",
    "            #convert each csv row into python dict\n",
    "            for row in csvReader: \n",
    "                #add this python dict to json array\n",
    "                if ',' in row['genome']: row['genome'] = row['genome'].replace(',', ';')\n",
    "                jsonArray.append(row)\n",
    "        \n",
    "        with open(jsonFilePath, 'w', encoding='utf-8') as jsonf: \n",
    "            jsonString = json.dumps(jsonArray, indent=4)\n",
    "            jsonf.write(jsonString)\n",
    "\n",
    "    csv_to_json(csvFilePath, jsonFilePath)\n",
    "\n",
    "for filename in os.listdir('json_fams'):\n",
    "    fam = filename.split('.')[0]\n",
    "    fpath = 'json_fams/' + filename\n",
    "    with open(fpath, 'r') as f:\n",
    "        fam_json = json.load(f)\n",
    "        for el in fam_json:\n",
    "            v = el['id']\n",
    "            if v not in data['data']: data['data'][v] = {}\n",
    "            for k in el.keys():\n",
    "                if k == 'id': continue\n",
    "                data['data'][v][k] = el[k]\n",
    "            data['data'][v]['family'] = fam\n",
    "            data['data'][v]['average_radius'] = data['data'][v].pop('radius')\n",
    "            \n",
    "with open('data/id_genus.txt', 'r') as f:\n",
    "    id_genus = {l.split()[0]:' '.join(l.split()[1:]) for l in f}\n",
    "for v in data['data']:\n",
    "    if v in id_genus:\n",
    "        data['data'][v]['genus'] = id_genus[v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c06107-8d25-414e-8677-5237a098061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write resolution data from ViperDB to data dictionary\n",
    "#the code used to make res.txt (containing resolution data) is no longer working\n",
    "\n",
    "with open(\"data/res.txt\", 'r') as f:\n",
    "    id_res = {l.split()[0]: l.split()[1] for l in f}\n",
    "for i in id_res:\n",
    "    data['data'][i]['resolution'] = id_res[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf35970d-cd18-4f12-8c06-e8fdcad5682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the gauge points from the point data\n",
    "gp_dict = {k:data['data'][k]['points'][0]['GP'] for k in data['data'].keys() if 'points' in data['data'][k].keys()}\n",
    "\n",
    "for k, v in gp_dict.items():\n",
    "    data['data'][k]['gauge_point'] = v\n",
    "\n",
    "data['gauge_points'] = {k:data['data'][k].pop('points') for k in data['data'] if 'points' in data['data'][k]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "83d1a310-4433-4b4b-ae2b-89839941aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename some ViperDB genomes for simplicity and consistency purposes\n",
    "genome_dict = {}\n",
    "genome_dict['dsRNA viruses'] = 'dsRNA'\n",
    "genome_dict['ssDNA viruses'] = 'ssDNA'\n",
    "genome_dict['Circular DNA'] = 'circular DNA'\n",
    "genome_dict['ssRNA viruses'] = 'ssRNA'\n",
    "genome_dict['sRNA'] = 'ssRNA'\n",
    "genome_dict['ssRNA positive-strand viruses; no DNA stage'] = 'ssRNA positive-strand viruses'\n",
    "genome_dict['ssRNA positive-strand viruse; no DNA stage'] = 'ssRNA positive-strand viruses'\n",
    "genome_dict['dsDNA viruses; no RNA stage'] = 'dsDNA'\n",
    "genome_dict['ssRNA-RT'] = 'ssRNA'\n",
    "genome_dict['ssRNA positive-strand viruses'] = 'ssRNA'\n",
    "genome_dict['ssRNA viruses; no DNA stage'] = 'ssRNA'\n",
    "genome_dict['ssRNA-RT'] = 'ssRNA'\n",
    "genome_dict['circular DNA'] = 'dsDNA'\n",
    "\n",
    "for v in data['data'].keys():\n",
    "    if 'genome' not in data['data'][v].keys(): continue\n",
    "    if isinstance(data['data'][v]['genome'], list): \n",
    "        if data['data'][v]['genome'] == ['dsDNA']:\n",
    "            data['data'][v]['genome'] = 'dsDNA'\n",
    "    elif data['data'][v]['genome'] in genome_dict:\n",
    "        data['data'][v]['genome'] = genome_dict[data['data'][v]['genome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b040b46-d40d-44fd-9901-e73fd84cdca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write aa data of full capsid to data dictionary (aas.json is made by count_aas.py)\n",
    "with open('aas.json', 'r') as f:\n",
    "    aa_dict = json.load(f)\n",
    "\n",
    "nw_aas = {}\n",
    "\n",
    "for v in aa_dict:\n",
    "    nw_aas[v[-4:]] = aa_dict[v]\n",
    "\n",
    "data['aas'] = {}\n",
    "\n",
    "for k, v in nw_aas.items():\n",
    "    if k not in data['data'].keys(): data['data'][k] = {}\n",
    "    data['aas'][k] = v\n",
    "    data['data'][k]['most_common_aa'] = max(v, key=v.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f80ae5-d7cc-4c6e-a439-e389eeafc1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write atoms data of capsid (id_atoms.txt is made by analyze_aas.ipynb)\n",
    "# excluding data if the capsid only has c alpha atoms (also in analyze_aas)\n",
    "with open('data/id_atoms.txt', 'r') as f:\n",
    "    id_atoms = {l.split()[0]:l.split()[1].replace('\\n', '') for l in f}\n",
    "with open('data/only_ca.txt', 'r') as f:\n",
    "    ca_ids = [l.replace('\\n', '') for l in f]\n",
    "\n",
    "for i in data['data']:\n",
    "    if 'atoms' in data['data'][i]:\n",
    "        del data['data'][i]['atoms']\n",
    "for i in id_atoms:\n",
    "    if i not in ca_ids:\n",
    "        data['data'][i]['atoms'] = int(id_atoms[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ba8b871-8e50-4fab-a822-6fc27991df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile data about the closest aa (& other aa within 5 Angstroms) for the closest point for each of the top 5 PAs in franken output\n",
    "import openpyxl\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = '../getvirons/xlfiles/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "from plot_util import *\n",
    "\n",
    "def letters(s):\n",
    "    valids = []\n",
    "    for character in s:\n",
    "        if character.isalpha():\n",
    "            valids.append(character)\n",
    "    return ''.join(valids)\n",
    "data['all_pas_closest'] = {}\n",
    "data['close_aas'] = {}\n",
    "for f in onlyfiles:\n",
    "    if 'full' not in f or '.xlsx' not in f: continue\n",
    "    xldir = mypath + f\n",
    "    wb = openpyxl.load_workbook(xldir)\n",
    "    v = f.split('_', 1)[-1].split('.')[0]\n",
    "    dclosest_aas = []\n",
    "    for sheetname in wb.sheetnames:\n",
    "        sh = wb[sheetname]\n",
    "        vitems = []\n",
    "        for row in sh.iter_rows():\n",
    "                vitems.append([cell.value for cell in row])\n",
    "        aas = []\n",
    "        cnt = 0\n",
    "        ds_aas = {}\n",
    "        for i in range(len(vitems[0])):\n",
    "            for j in range(2, len(vitems)):\n",
    "                if vitems[j][i] is not None and '.' in str(vitems[j][i]):\n",
    "                    d = vitems[j][i]\n",
    "                    ds_aas[d] = vitems[j][i:i+4]\n",
    "                    if d < 5:\n",
    "                        aas.append(letters(vitems[j][i+2]))\n",
    "                        if vitems[j][i+3] != 'N/A': aas.append(letters(vitems[j][i+3]))\n",
    "        closest_aa = letters(ds_aas[min(ds_aas.keys())][2])\n",
    "        other_aa = letters(ds_aas[min(ds_aas.keys())][3])\n",
    "        dclosest_aas.append([closest_aa, other_aa])\n",
    "    data['all_pas_closest'][v] = dclosest_aas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3147807e-4783-4839-84a1-7b4fda7d404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile data about the closest aa (& other aa within 5 Angstroms) for the closest point for the single top line in franken output, along with other aa data described in the README\n",
    "mypath = '../getvirons/xlfiles/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "idpath = '../getvirons/xlfiles_prechain/'\n",
    "idpafiles = [f for f in listdir(idpath) if isfile(join(idpath, f))]\n",
    "id_pa = {}\n",
    "for f in idpafiles:\n",
    "    try:\n",
    "        if 'full' in f: continue\n",
    "        v = f.split('.xlsx')[0]\n",
    "        xldir = idpath + f\n",
    "        wb = openpyxl.load_workbook(xldir)\n",
    "        name = wb.sheetnames[0]\n",
    "        sh = wb[name]\n",
    "        r = sh[2]\n",
    "        closest_pa = r[0].value\n",
    "        id_pa[v] = closest_pa\n",
    "    except Exception as e:\n",
    "        print(v)\n",
    "\n",
    "with open('id_pa.txt', 'w') as f:\n",
    "    for i in id_pa:\n",
    "        f.write(i + ' ' + str(id_pa[i])+ '\\n')\n",
    "\n",
    "data['close_aas'] = {}\n",
    "for f in onlyfiles:\n",
    "    if 'full' not in f or '.xlsx' not in f: continue\n",
    "    xldir = mypath + f\n",
    "    wb = openpyxl.load_workbook(xldir)\n",
    "    v = f.split('_', 1)[-1].split('.')[0]\n",
    "    pa = id_pa[v]\n",
    "    pa_sh = f\"{v}_pa_{pa}\"\n",
    "    resdict = []\n",
    "    sh = wb[pa_sh]\n",
    "    vitems = []\n",
    "    for row in sh.iter_rows():\n",
    "            vitems.append([cell.value for cell in row])\n",
    "    aas = []\n",
    "    ds_aas = {}\n",
    "    gp_aas = {}\n",
    "    for j in range(2, len(vitems)):\n",
    "        for i in range(len(vitems[0])):\n",
    "            if vitems[j][i] is not None and '.' in str(vitems[j][i]):\n",
    "                d = vitems[j][i]\n",
    "                ds_aas[d] = vitems[j][i:i+4]\n",
    "                if j == 2:\n",
    "                    gp_aas[d] = ds_aas[d]\n",
    "                if d < 5:\n",
    "                    aas.append(letters(vitems[j][i+2]))\n",
    "                    if vitems[j][i+3] != 'N/A': aas.append(letters(vitems[j][i+3]))\n",
    "    resdict.extend(aas)\n",
    "    closest_gp_aa = letters(gp_aas[min(gp_aas.keys())][2])\n",
    "    other_gp_aa = letters(gp_aas[min(gp_aas.keys())][3])\n",
    "    closest_aa = letters(ds_aas[min(ds_aas.keys())][2])\n",
    "    other_aa = letters(ds_aas[min(ds_aas.keys())][3])\n",
    "    data['data'][v]['closest_gp_aa'] = closest_gp_aa\n",
    "    data['data'][v]['other_gp_aa'] = other_gp_aa\n",
    "    data['data'][v]['closest_aa'] = closest_aa\n",
    "    data['data'][v]['other_aa'] = other_aa\n",
    "    cres = countlist(resdict)\n",
    "    data['close_aas'][v] = cres\n",
    "    try:\n",
    "        data['data'][v]['common_gauge_aa'] = max(cres, key=cres.get)\n",
    "    except Exception as e:\n",
    "        #print(v)\n",
    "        pass\n",
    "for f in onlyfiles:\n",
    "    if 'full' not in f: continue\n",
    "    xldir = mypath + f\n",
    "    wb = openpyxl.load_workbook(xldir)\n",
    "    v = f.split('_', 1)[-1].split('.')[0]\n",
    "    resdict = []\n",
    "    for name in wb.sheetnames:\n",
    "            sh = wb[name]\n",
    "            vitems = []\n",
    "            for row in sh.iter_rows():\n",
    "                    vitems.append([cell.value for cell in row])\n",
    "            aas = []\n",
    "            cnt = 0\n",
    "            for i in range(len(vitems[0])):\n",
    "                for j in range(2, len(vitems)):\n",
    "                    if vitems[j][i] is not None and '.' in str(vitems[j][i]):\n",
    "                        d = vitems[j][i]\n",
    "                        if d < 5:\n",
    "                            aas.append(letters(vitems[j][i+2]))\n",
    "                            if vitems[j][i+3] != 'N/A': aas.append(letters(vitems[j][i+3]))\n",
    "            resdict.extend(aas)\n",
    "    cres = countlist(resdict)\n",
    "    data['close_aas'][v] = cres\n",
    "    data['data'][v]['common_gauge_aa'] = max(cres, key=cres.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3260922-8965-424f-a97e-44894ca4be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate into discrete and continuous properties, change data types accordingly, and add the lists to dictionary\n",
    "for v in data['data']:\n",
    "    if 'point_arrays' in data['data'][v]: del data['data'][v]['point_arrays']\n",
    "\n",
    "if 'gauge_points' in data: del data['gauge_points']\n",
    "data['data']['7wqp']['genome'] = 'NA'\n",
    "properties = {}\n",
    "properties['discrete'] = []\n",
    "properties['continuous'] = []\n",
    "\n",
    "def num(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return float(s)\n",
    "\n",
    "var_dict = {}\n",
    "for k in data['data']:\n",
    "    if 'tnum' in data['data'][k].keys(): del data['data'][k]['tnum']\n",
    "    for v in data['data'][k]:\n",
    "        if v not in var_dict.keys(): var_dict[v] = 'continuous'\n",
    "        if not str(data['data'][k][v]).replace('.','',1).isdigit() and len(str(data['data'][k][v])) > 0: \n",
    "            var_dict[v] = 'discrete'\n",
    "\n",
    "conts = [k for k,v in var_dict.items() if v=='continuous']\n",
    "discs = [k for k,v in var_dict.items() if v=='discrete']\n",
    "discs.append('gauge_point')\n",
    "conts.remove('gauge_point')\n",
    "properties['continuous'] = conts\n",
    "properties['discrete'] = discs\n",
    "\n",
    "for virus in data['data']:\n",
    "    for c in conts:\n",
    "        if c in data['data'][virus].keys():\n",
    "            item = str(data['data'][virus][c])\n",
    "            if len(item) == 0: del data['data'][virus][c]\n",
    "            else: data['data'][virus][c] = num(item)\n",
    "\n",
    "data['properties'] = properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d295df-5679-47c5-a925-50c061479a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create filters for sorting by discrete variables\n",
    "import numpy as np\n",
    "ks = {}\n",
    "for v in data['data']:\n",
    "    for k in data['data'][v]:\n",
    "        if k not in data['properties']['discrete']: continue\n",
    "        if k not in ks: ks[k] = []\n",
    "        if data['data'][v][k] == '': data['data'][v][k] = 'NA'\n",
    "        if str(data['data'][v][k]) not in ks[k]: ks[k].append(str(data['data'][v][k]))\n",
    "for f in ks:\n",
    "    ks[f] = sorted(ks[f])\n",
    "ks['gauge_point'] = [str(s) for s in sorted([int(p) for p in ks['gauge_point']])]\n",
    "trank = [int(''.join([s for s in t if s.isdigit()])) if len([s for s in t if s.isdigit()]) > 0 else 999 for t in ks['tnumber']]\n",
    "tsort = np.argsort(trank)\n",
    "ts = [0]*20\n",
    "for i in range(len(tsort)):\n",
    "    el = ks['tnumber'][tsort[i]]\n",
    "    ts[i] = el\n",
    "ks['tnumber'] = ts\n",
    "data['filters'] = ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1771b823-7635-4a5b-985a-293ddf6315c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/db.json', 'w') as f:\n",
    "    json_str = json.dumps(data, indent=4)\n",
    "    f.write(json_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
